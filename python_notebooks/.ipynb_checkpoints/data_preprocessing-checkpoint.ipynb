{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57f6346-1c76-482b-8069-9f4321e5aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "datadir = '../Collected Data/'\n",
    "test_participant = 20\n",
    "\n",
    "def read_file(num):\n",
    "    dataset = pd.read_csv(datadir+'Collected Labeled Data - Phase 01/participant '+ num +'.csv')\n",
    "    if 'Time' in dataset.columns:\n",
    "        dataset.drop('Time', axis=1, inplace=True)\n",
    "    if 'Start' in dataset.columns:\n",
    "        dataset.drop('Start', axis=1, inplace=True)\n",
    "    if 'End' in dataset.columns:\n",
    "        dataset.drop('End', axis=1, inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b19e8-fee6-4393-8996-921178ea180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slope(data):\n",
    "    data = data.replace(np.inf, np.nan).replace(-np.inf, np.nan).dropna()\n",
    "    x = np.arange(len(data))\n",
    "    if len(x)>1:\n",
    "        slope = np.polyfit(x, data, 1)[0]\n",
    "    else: \n",
    "        slope = 0\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe0062a4-2df6-490d-b647-bae102c57fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_crossing(avg_value, pressure_values, flag):\n",
    "    if(flag):\n",
    "        count = len([x for x in pressure_values if x < avg_value])  #This will return the percentage of values below average\n",
    "        return round(count/len(pressure_values), 2)\n",
    "    else:\n",
    "        count = len([x for x in pressure_values if x > avg_value]) #This will return the percentage of values above average\n",
    "        return round(count/len(pressure_values), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c64c37-a405-437a-9130-50e2a4fc48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_interval(group):\n",
    "    avg_pressure = group['Pressure'].mean()\n",
    "    summary = {\n",
    "        'avg_accX': group['X'].mean(),\n",
    "        'min_accX': group['X'].min(),\n",
    "        'max_accX': group['X'].max(),\n",
    "        'var_accX': group['X'].var(),\n",
    "        'std_accX': np.std(group['X']),\n",
    "        'avg_accY': group['Y'].mean(),\n",
    "        'min_accY': group['Y'].min(),\n",
    "        'max_accY': group['Y'].max(),\n",
    "        'var_accY': group['Y'].var(),\n",
    "        'std_accY': np.std(group['Y']),\n",
    "        'avg_accZ': group['Z'].mean(),\n",
    "        'min_accZ': group['Z'].min(),\n",
    "        'max_accZ': group['Z'].max(),\n",
    "        'var_accZ': group['Z'].var(),\n",
    "        'std_accZ': np.std(group['Z']),\n",
    "        'avg_magnitude': group['Magnitude'].mean(),\n",
    "        'min_magnitude': group['Magnitude'].min(),\n",
    "        'max_magnitude': group['Magnitude'].max(),\n",
    "        'var_magnitude': group['Magnitude'].var(),\n",
    "        'std_magnitude': np.std(group['Magnitude']),\n",
    "        'avg_pressure': avg_pressure,\n",
    "        'min_pressure': group['Pressure'].min(),\n",
    "        'max_pressure': group['Pressure'].max(),\n",
    "        'var_pressure': group['Pressure'].var(),\n",
    "        'range_pressure': (group['Pressure'].max() - group['Pressure'].min()),\n",
    "        'std_pressure': np.std(group['Pressure']),\n",
    "        'slope_pressure': calculate_slope(group['Pressure']),\n",
    "        'kurtosis_pressure': kurtosis(group['Pressure']),\n",
    "        'Pressure_below_avg': avg_crossing(avg_pressure, group['Pressure'], 1), #flag 1 to calulate below average values\n",
    "        'Pressure_above_avg': avg_crossing(avg_pressure, group['Pressure'], 0), #flag 0 to calculate above average values\n",
    "        'skew_pressure': skew(group['Pressure']),\n",
    "        'Label': group['Label'].mode().iloc[0]  # Most frequent label in the interval\n",
    "    }\n",
    "    return pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "135138c0-2d18-4073-96e1-bfd9f63dd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(num, dataset):\n",
    "    # Applying the summarize_interval function to each 2-second interval\n",
    "    interval_seconds = 2\n",
    "    result_df = dataset.groupby(dataset['Timestamp'] // interval_seconds).apply(summarize_interval)\n",
    "    \n",
    "    # Reset index and drop the Timestamp column (if needed)\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Step 5: Save the preprocessed data to a new CSV file\n",
    "    result_df.to_csv(datadir+'preprocessed/'+'preprocessed_data'+str(num)+'.csv', index=False)\n",
    "    result_df.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b324c72-6eee-407a-8967-6ef76aa865e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_7444\\1591286499.py:31: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis_pressure': kurtosis(group['Pressure']),\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_7444\\1591286499.py:34: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew_pressure': skew(group['Pressure']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for i in range(1, 21):\n",
    "    if i != test_participant:\n",
    "        preprocessing(i, read_file(f\"0{i}\" if i < 9 else str(i)))\n",
    "\n",
    "dir = (datadir+'preprocessed/'+str(test_participant)+'/')\n",
    "csv_files = glob.glob(datadir+'preprocessed/'+'*.csv')\n",
    "print(len(csv_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8e97e04-b064-4897-a843-efa4a4757548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Collected Data/preprocessed\\preprocessed_data1.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data10.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data11.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data12.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data13.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data14.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data15.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data16.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data17.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data18.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data19.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data2.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data3.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data4.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data5.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data6.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data7.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data8.csv\n",
      "../Collected Data/preprocessed\\preprocessed_data9.csv\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "csv_files = csv_files[:19]\n",
    "# Loop through each CSV file and append its DataFrame to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, converters={'Label': lambda x: x.lower() if x not in ['Null', 'null', ' null'] else x})\n",
    "    dfs.append(df)\n",
    "    print(file)\n",
    "# Concatenate all DataFrames in the list along rows (axis=0)\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "405e72ed-e65f-4d1c-a5d5-cf0faef2815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(datadir+'preprocessed/'+'preprocessed_traindata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "488ae0b3-9d17-475b-8561-dfc17be4726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(test_participant, read_file(f\"0{test_participant}\" if test_participant < 9 else str(test_participant)))\n",
    "newdf = pd.read_csv(datadir+'preprocessed/'+'preprocessed_data'+str(test_participant)+'.csv', converters={'Label': lambda x: x.lower() if x not in ['Null', 'null', ' null'] else x})\n",
    "newdf.to_csv(datadir+'preprocessed/'+'preprocessed_testdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95638555-0688-47b8-a103-b21b3146b4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
