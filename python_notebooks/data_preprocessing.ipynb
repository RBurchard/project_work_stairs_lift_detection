{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57f6346-1c76-482b-8069-9f4321e5aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import Counter\n",
    "\n",
    "datadir = '../Collected Data/'\n",
    "test_participant = 20\n",
    "\n",
    "def read_file(num):\n",
    "    dataset = pd.read_csv(datadir+'Collected Updated Labeled Data - Phase 01/participant '+ num +'.csv')\n",
    "    if 'Time' in dataset.columns:\n",
    "        dataset.drop('Time', axis=1, inplace=True)\n",
    "    if 'Start' in dataset.columns:\n",
    "        dataset.drop('Start', axis=1, inplace=True)\n",
    "    if 'End' in dataset.columns:\n",
    "        dataset.drop('End', axis=1, inplace=True)\n",
    "        \n",
    "    strings_to_replace = ['null', ' null']\n",
    "    replacement_string = 'Null'\n",
    "    dataset = dataset.replace(strings_to_replace, replacement_string)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5596884f-56cf-4c58-8c5b-1e45ddc65d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.285714285714285\n",
      "19.047619047619047\n",
      "19.047619047619047\n",
      "33.33333333333333\n",
      "14.285714285714285\n",
      "Counter({4: 7, 2: 4, 3: 4, 1: 3, 5: 3})\n"
     ]
    }
   ],
   "source": [
    "l= [1,2,3,4,1,2,3,3,1,2,4,4,5,4,5,4,5,4,4,3,2]\n",
    "d = Counter(l)\n",
    "n = len(l)\n",
    "for each in set(l):\n",
    "    print((d[each]/n)*100)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090b19e8-fee6-4393-8996-921178ea180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slope(data):\n",
    "    data = data.replace(np.inf, np.nan).replace(-np.inf, np.nan).dropna()\n",
    "    x = np.arange(len(data))\n",
    "    if len(x)>1:\n",
    "        slope = np.polyfit(x, data, 1)[0]\n",
    "    else: \n",
    "        slope = 0\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0062a4-2df6-490d-b647-bae102c57fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_crossing(avg_value, pressure_values, flag):\n",
    "    if(flag):\n",
    "        count = len([x for x in pressure_values if x < avg_value])  #This will return the percentage of values below average\n",
    "        return round(count/len(pressure_values), 2)\n",
    "    else:\n",
    "        count = len([x for x in pressure_values if x > avg_value]) #This will return the percentage of values above average\n",
    "        return round(count/len(pressure_values), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c64c37-a405-437a-9130-50e2a4fc48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_interval(group):\n",
    "    avg_pressure = group['Pressure'].mean()\n",
    "    label_mode = group['Label'].mode().iloc[0] #calculating mode value to assign label\n",
    "    labeldict = Counter(group['Label']) #getting the count of each value in label\n",
    "    label_percentage = (labeldict[label_mode] / len(group['Label']) ) * 100 #calculating the percentage of values that are same as mode\n",
    "    #print(f\"label is {label_mode} and label Percentage is {label_percentage}\")\n",
    "    if ((label_percentage > 79) and (label_mode not in ['Null','null',' null','\\tnull'])): #calculating the values only if the label is atleast 80% else return \n",
    "        summary = {\n",
    "            'avg_accX': group['X'].mean(),\n",
    "            'min_accX': group['X'].min(),\n",
    "            'max_accX': group['X'].max(),\n",
    "            'var_accX': group['X'].var(),\n",
    "            'std_accX': np.std(group['X']),\n",
    "            'avg_accY': group['Y'].mean(),\n",
    "            'min_accY': group['Y'].min(),\n",
    "            'max_accY': group['Y'].max(),\n",
    "            'var_accY': group['Y'].var(),\n",
    "            'std_accY': np.std(group['Y']),\n",
    "            'avg_accZ': group['Z'].mean(),\n",
    "            'min_accZ': group['Z'].min(),\n",
    "            'max_accZ': group['Z'].max(),\n",
    "            'var_accZ': group['Z'].var(),\n",
    "            'std_accZ': np.std(group['Z']),\n",
    "            'avg_magnitude': group['Magnitude'].mean(),\n",
    "            'min_magnitude': group['Magnitude'].min(),\n",
    "            'max_magnitude': group['Magnitude'].max(),\n",
    "            'var_magnitude': group['Magnitude'].var(),\n",
    "            'std_magnitude': np.std(group['Magnitude']),\n",
    "            'avg_pressure': avg_pressure,\n",
    "            'min_pressure': group['Pressure'].min(),\n",
    "            'max_pressure': group['Pressure'].max(),\n",
    "            'var_pressure': group['Pressure'].var(),\n",
    "            'range_pressure': (group['Pressure'].max() - group['Pressure'].min()),\n",
    "            'std_pressure': np.std(group['Pressure']),\n",
    "            'slope_pressure': calculate_slope(group['Pressure']),\n",
    "            'kurtosis_pressure': kurtosis(group['Pressure']),\n",
    "            'Pressure_below_avg': avg_crossing(avg_pressure, group['Pressure'], 1), #flag 1 to calulate below average values\n",
    "            'Pressure_above_avg': avg_crossing(avg_pressure, group['Pressure'], 0), #flag 0 to calculate above average values\n",
    "            'skew_pressure': skew(group['Pressure']),\n",
    "            'Label': label_mode,  # Most frequent label in the interval\n",
    "        }\n",
    "        return pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135138c0-2d18-4073-96e1-bfd9f63dd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(num, dataset):\n",
    "    # Applying the summarize_interval function to each 2-second interval\n",
    "    interval_seconds = 2\n",
    "    result_df = dataset.groupby(dataset['Timestamp'] // interval_seconds).apply(summarize_interval)\n",
    "    # Reset index to flatten the DataFrame and remove the multi-index\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    # Remove rows with any NaN values\n",
    "    result_df = result_df.dropna(how='any')\n",
    "    #save the preprocessed data to a new CSV file\n",
    "    result_df.to_csv(datadir+'preprocessed/'+str(test_participant)+'/preprocessed_data'+str(num)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b324c72-6eee-407a-8967-6ef76aa865e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NiMou\\AppData\\Local\\Temp\\ipykernel_3044\\2307463476.py:36: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'kurtosis_pressure': kurtosis(group['Pressure']),\n",
      "C:\\Users\\NiMou\\AppData\\Local\\Temp\\ipykernel_3044\\2307463476.py:39: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'skew_pressure': skew(group['Pressure']),\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for i in range(1, 21):\n",
    "    if i != test_participant:\n",
    "        preprocessing(i, read_file(f\"0{i}\" if i < 10 else str(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be77e03d-82bc-4591-a4be-7c18bfb6cdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob(datadir+'preprocessed/'+str(test_participant)+'/*.csv')\n",
    "print(len(csv_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8e97e04-b064-4897-a843-efa4a4757548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Collected Data/preprocessed/20\\preprocessed_data1.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data10.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data11.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data12.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data13.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data14.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data15.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data16.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data17.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data18.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data19.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data2.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data3.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data4.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data5.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data6.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data7.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data8.csv\n",
      "../Collected Data/preprocessed/20\\preprocessed_data9.csv\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "csv_files = csv_files[:19]\n",
    "# Loop through each CSV file and append its DataFrame to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file,converters={'Label': lambda x: x.lower() if x not in ['Null', 'null', ' null', '\\tnull'] else 'Null'})\n",
    "    dfs.append(df)\n",
    "    print(file)\n",
    "# Concatenate all DataFrames in the list along rows (axis=0)\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405e72ed-e65f-4d1c-a5d5-cf0faef2815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(datadir+'preprocessed/'+str(test_participant)+'/preprocessed_traindata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488ae0b3-9d17-475b-8561-dfc17be4726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(test_participant, read_file(f\"0{test_participant}\" if test_participant < 10 else str(test_participant)))\n",
    "newdf = pd.read_csv(datadir+'preprocessed/'+str(test_participant)+'/preprocessed_data'+str(test_participant)+'.csv', converters={'Label': lambda x: x.lower() if x not in ['Null', 'null', ' null'] else 'Null'})\n",
    "newdf.to_csv(datadir+'preprocessed/'+str(test_participant)+'/preprocessed_testdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95638555-0688-47b8-a103-b21b3146b4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
